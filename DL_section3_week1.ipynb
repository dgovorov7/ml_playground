{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2640b7",
   "metadata": {},
   "source": [
    "# ML strategy\n",
    "\n",
    "this section goes over how you should approach a ML problem and strategies to figure out the most promising things to try and change\n",
    "\n",
    "# Orthogonalization\n",
    "\n",
    "The goal is to design hyper parameters that only change 1 thing, or only affect 1 part of the desired outcome. \n",
    "The hope is that in separating out parameters it would make tuning easier and more interpretable\n",
    "\n",
    "Each issue with your model should have its own set of hyper parameters that can be tuned to reslove that one issue; such issues and parameters could be:\n",
    "- doesn't fit Training Set Well\n",
    "    - change number of layers\n",
    "    - change optimization algorithm\n",
    "- doesn't fit dev set well\n",
    "    - apply Regularization techniques\n",
    "- doesn't fit test set well\n",
    "    - bigger dev set\n",
    "- doesn't preforms well in real world\n",
    "    - change dev set or cost function: these might not be a good reflecting of the real world problem you are trying to solve\n",
    "\n",
    "\n",
    "If a single piece of your ML system is going wrong, you have parameters that you can use to tune it that will only affect the outcome of that metric. \n",
    "\n",
    "Parameters such as early stopping simultaneously affects the training set and dev set performance, its always an option to use but is not orthogonal. Meaning that changing the early stop time affects both the training set and dev set metrics. tuning it may make your dev set fit better but your training set preform worse, meaning you have to now retune your training set parameters again, waisting time.\n",
    "\n",
    "\n",
    "# Single Number Evaluation Metric\n",
    "\n",
    "- precision -> of examples recognized, what percent were correct\n",
    "- recall -> what percent of real data does it recognize correctly\n",
    "\n",
    "Classifier A does better on precision, but classifier B does better on recall; Which one is preforming better?\n",
    "\n",
    "Its a good idea to have a single number metric that you can use for comparison, something that would combine both the precision and recall of a model to give a single value that is easy to compare such as an F1 score:\n",
    "\n",
    "https://en.wikipedia.org/wiki/F-score\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
