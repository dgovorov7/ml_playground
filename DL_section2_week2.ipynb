{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b940a1f",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "ML is iterative process, having good optimization algorithms allows for faster iterations thus faster results\n",
    "\n",
    "better optimize convergence of model -> faster results\n",
    "\n",
    "Overview:\n",
    "- Mini-batches\n",
    "- Exponentially weighted averages / Moving average + bias correction\n",
    "- Adam Optimization\n",
    "- learning rate decay\n",
    "\n",
    "## Mini-batch\n",
    "\n",
    "![img](https://miro.medium.com/1*I2BkYWA_OJzHWFo6kc0fmQ.jpeg)\n",
    "\n",
    "vectorization allows for faster processing of batches\n",
    "\n",
    "if m = 5,000,000 (examples) it will take a while to run a single forward and back pass\n",
    "\n",
    "to speed up make mini batches ~1000 examples\n",
    "\n",
    "X{500} = 500 mini-batches each containing 10,000 examples of original 5,000,000 example dataset\n",
    "\n",
    "\n",
    "1 epoch = 1 pass through the whole data set, usual you want to take multiple passes through the dataset\n",
    "\n",
    "![img](https://media.licdn.com/dms/image/v2/C4D12AQGhX6hIG-vdVw/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1520236996533?e=1763596800&v=beta&t=1Vdp9pfcK78D0_wvC9gkRSFuOpF5w5-B6Bp6bFAh0Hc)\n",
    "\n",
    "Mini batch is more noisy, sometimes loss goes up, but over time it will trend down. Some batches may be easier than others\n",
    "\n",
    "mini-batch size is a hyperparameter\n",
    "- Large mini-batch size -> regular batch;  low noise but takes alot of time for a single iteration\n",
    "- mini-batch size -> stochastic gradient descent, looks at only 1 example at a time; very noisy but very fast\n",
    "\n",
    "What works best is something inbetween, this value can be tunned like any other hyper parameter\n",
    "\n",
    "Smaller training data sets can use whole batch \n",
    "\n",
    "typical minibatch sizes are 2^n (128,256,512)\n",
    "\n",
    "Make sure it fits in CPU/GPU memory\n",
    "\n",
    "\n",
    "## Exponentially weighted averages / Moving average\n",
    "\n",
    "Compute average \n",
    "\n",
    "V[t] = B*V[t-1] + (1-B)Theta[t]\n",
    "\n",
    "V[t] = average temperature over 1 / (B-1) Timestamps\n",
    "\n",
    "Larger B(eta) :\n",
    "- B = 0.98 averages over about 50 timestamps\n",
    "- Produces smoother curve\n",
    "- introduces latency as estimate changes gradually\n",
    "\n",
    "Smaller B(eta) :\n",
    "- B = 0.5 averages over about 2 timestamps\n",
    "\n",
    "- more noise\n",
    "- adapts faster to change\n",
    "\n",
    "Provides a smooth trend estimate without storing long history; widely used in optimization algorithms for tracking moving averages of gradients or squared gradients.\n",
    "\n",
    "```python\n",
    "\n",
    "V = 0\n",
    "data = []\n",
    "weighted_averages = [V]\n",
    "\n",
    "for x in data:\n",
    "    V = beta * V + (1 - beta) * x\n",
    "    weighted_averages.append(V)\n",
    "    print(V)\n",
    "```\n",
    "\n",
    "Very memory and computationally cheap to compute\n",
    "\n",
    "### Bias Correction\n",
    "\n",
    "V[0] is initialized to 0, meaning that the first few datapoints will have a bias towards 0\n",
    "\n",
    "additional operation of scaling up our weighter average for timestamp T using `V = V / ((1-beta)**t)`\n",
    "\n",
    "this causes:\n",
    "- for moving average with large value of beta -> V will be dramatically scaled up at small values of t\n",
    "- for moving averages with small values of beta -> V will be less scaled up at small values of t\n",
    "\n",
    "```python\n",
    "\n",
    "V = 0\n",
    "data = []\n",
    "weighted_averages = [V]\n",
    "\n",
    "for t,x in enumerate(data):\n",
    "    V = (beta * V) + ((1 - beta) * x)\n",
    "    V = V / ((1-beta)**t)\n",
    "    weighted_averages.append(V)\n",
    "    print(V)\n",
    "```\n",
    "```python\n",
    "\n",
    "V = 0\n",
    "data = []\n",
    "weighted_averages = [V]\n",
    "\n",
    "for t,x in enumerate(data):\n",
    "    V_previous = beta * V\n",
    "    V_current = (1-beta)* x\n",
    "    V = V_previous + V_current\n",
    "    V = V / ((1-beta)**t)\n",
    "    weighted_averages.append(V)\n",
    "    print(V)\n",
    "```\n",
    "Now when t is near 0 the previous averages will have near 0 affect, and as t increases it will have more affect on averages\n",
    "\n",
    "\n",
    "# Gradient Descent  with momentum\n",
    "\n",
    "\n",
    "Compute Exponentially weighted average of past gradients to calculate how much to tune each parameter by with goal of smoothing out any noise for faster convergence\n",
    "\n",
    "algorithm maintains a moving average of the gradient vector and uses the weighted average to tune the parameters instead of the raw gradients\n",
    "\n",
    "\n",
    "```python\n",
    "Vdw,Vdb = 0\n",
    "for i in iteration:\n",
    "    dw,db = backprop()\n",
    "\n",
    "    Vdw = beta * Vdw + (1-beta)* dw\n",
    "    Vdb = beta * Vdb + (1-beta)* db\n",
    "\n",
    "    w -= learning_rate * Vdw\n",
    "    b -= learning_rate * Vdb\n",
    "```\n",
    "\n",
    "commonly used value for beta is 0.9: averages last 10 gradients\n",
    "\n",
    "In practice bias correction isn't necessary as after 10 (beta=0.9) iterations the bias towards 0 would have disappeared\n",
    "\n",
    "sometimes `Vdw = beta * Vdw + dw` is used (1-beta) omitted ; causes Vdw value to be scaled by (1-beta), causing beta to affect learning_rate if beta is changed\n",
    "\n",
    "# RMSprop\n",
    "\n",
    "Root Mean Square Propagation\n",
    "\n",
    "keep track of a moving average of squared gradients\n",
    "\n",
    "```python\n",
    "\n",
    "Sdw,Sdb = 0\n",
    "epsilon = 1e-8 # numerical stability for if Sdw is very small\n",
    "for i in iteration:\n",
    "    dw,db = backprop()\n",
    "\n",
    "    Sdw = beta * Sdw + (1-beta)* (dw**2)\n",
    "    Sdb = beta * Sdb + (1-beta)* (db**2)\n",
    "\n",
    "    w -= learning_rate * (dw / np.sqrt(Sdw) + epsilon)\n",
    "    b -= learning_rate * (db / np.sqrt(Sdb) + epsilon)\n",
    "```\n",
    "\n",
    "Parameters with consistently large gradients get smaller effective learning rates\n",
    "\n",
    "Parameters with small or stable gradients get larger effective learning rates\n",
    "\n",
    "This helps to reduce oscillations for steep directions while maintaining a fast learning rate for shallow directions\n",
    "\n",
    "Allows for faster learning rates with less risk of divergence\n",
    "\n",
    "# Adam Optimization Algo\n",
    "\n",
    "takes momentum and RMSprop and combines\n",
    "\n",
    "\n",
    "```python\n",
    "Sdw,Sdb,Vdw,Vdb = 0\n",
    "\n",
    "epsilon = 1e-8 # numerical stability for if gradient is very small, doesn't affect performance \n",
    "\n",
    "learning_rate = 0.001 # needs to be tuned\n",
    "\n",
    "# these hyper params are often not tuned\n",
    "beta_1 = 0.9 # Momentum beta\n",
    "beta_2 = 0.999 # RMSProp beta\n",
    "\n",
    "for i in iteration:\n",
    "    dw,db = backprop()\n",
    "\n",
    "    #RMSprop\n",
    "    Sdw = beta_2 * Sdw + (1-beta_2)* (dw**2)\n",
    "    Sdb = beta_2 * Sdb + (1-beta_2)* (db**2)\n",
    "\n",
    "    #RMSprop bias correction\n",
    "    Sdw_corrected = Sdw / (1-(beta_2**i))\n",
    "    Sdb_corrected = Sdb / (1-(beta_2**i))\n",
    "\n",
    "    #Momentum\n",
    "    Vdw = beta_1 * Vdw + (1-beta_1)* dw\n",
    "    Vdb = beta_1 * Vdb + (1-beta_1)* db\n",
    "\n",
    "    #Momentum bias correction\n",
    "    Vdw_corrected = Vdw / (1-(beta_1**i))\n",
    "    Vdb_corrected = Vdb / (1-(beta_1**i))\n",
    "\n",
    "    #here we combined RMSprop and Momentum\n",
    "    w -= learning_rate * (Vdw_corrected / np.sqrt(Sdw_corrected) + epsilon)\n",
    "    b -= learning_rate * (Vdb_corrected / np.sqrt(Sdb_corrected) + epsilon)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "# learning rate decay\n",
    "\n",
    "slowly reduce learning_rate during training\n",
    "\n",
    "Assumption is that in initial iterations we can take larger steps, but in later iterations we should take smaller steps as we converge to the local minima\n",
    "\n",
    "\n",
    "```python\n",
    "# 2 hyper parameters\n",
    "decay_rate = \n",
    "initial_learning_rate\n",
    "\n",
    "learning_rate = 1 / (1+decay_rate * epoch_number) * initial_learning_rate\n",
    "```\n",
    "### alternative methods\n",
    "exponetial decay: `learning_rate = 0.95**(epoch_number) * initial_learning_rate`\n",
    "\n",
    "`learning_rate = (some_constant / np.sqrt(epoch_num) ) * init_learning_rate `\n",
    "\n",
    "Manual decay: for some long running times, you can just manually lessen learning rate when you see fit\n",
    "\n",
    "Fixed Interval Scheduling: at specific timestamps use specifically defined learning rates\n",
    "\n",
    "scheduled lr delay: `learning_rate = (1/ (1 + decay_rate * np.floor(epoch_num / time_interval))) * learning_rate0`\n",
    "\n",
    "\n",
    "# Local Optima\n",
    "\n",
    "![img](https://www.allaboutlean.com/wp-content/uploads/2018/08/Local-Global-Optimum.png)\n",
    "\n",
    "Thinking about local optimas on a low dimensional space would lead us to assume there are many local minimas \n",
    "\n",
    "Most critical points in high dimensional space are saddle points and not local minimas due to the chance that every dimension (2000 dimensional space) at a critical point is a minima is very low\n",
    "\n",
    "thus unlikely to get stuck in a bad local optima for models with many parameters\n",
    "\n",
    "Plateaus, a region where the gradient is near 0, takes a while to escape. this is why optimizers like adam are useful\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
